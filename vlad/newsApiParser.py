from dotenv import load_dotenv
import os
import requests
from datetime import datetime, timedelta, timezone
import csv
from pathlib import Path
import feedparser

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv(dotenv_path="config/.env")
NEWS_API_KEY = os.getenv("NEWS_API_KEY")

# –ü–µ—Ä–∏–æ–¥ –Ω–æ–≤–æ—Å—Ç–µ–π
to_date = datetime.now(timezone.utc)
from_date = to_date - timedelta(days=14)
from_str = from_date.strftime("%Y-%m-%d")
to_str = to_date.strftime("%Y-%m-%d")

# --- –ü–∞—Ä—Å–∏–Ω–≥ –∏–∑ NewsAPI ---
print("üîé –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ NewsAPI...")
newsapi_articles = []
url = "https://newsapi.org/v2/everything"
params = {
    "q": "–ú–µ–≥–∞–§–æ–Ω",
    "language": "ru",
    "sortBy": "publishedAt",
    "from": from_str,
    "to": to_str,
    "pageSize": 100,
    "page": 1,
    "apiKey": NEWS_API_KEY
}

while True:
    response = requests.get(url, params=params)
    data = response.json()

    if data.get("status") != "ok":
        print("‚ùå –û—à–∏–±–∫–∞:", data.get("message"))
        break

    articles = data.get("articles", [])
    if not articles:
        break

    for article in articles:
        title = article["title"]
        source = article["source"]["name"]
        url_ = article["url"]
        published_at = article["publishedAt"]

        dt = datetime.fromisoformat(published_at.rstrip("Z"))
        formatted_date = dt.strftime("%d.%m.%Y %H:%M")

        newsapi_articles.append([formatted_date, source, title, url_])

    print(f"  ‚ûï –°—Ç—Ä–∞–Ω–∏—Ü–∞ {params['page']} ‚Äî –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π.")
    if len(articles) < 100:
        break

    params["page"] += 1

# --- –ü–∞—Ä—Å–∏–Ω–≥ –∏–∑ Google News RSS ---
print("üîé –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ Google News RSS...")
google_news_articles = []
rss_url = "https://news.google.com/rss/search?q=–ú–µ–≥–∞–§–æ–Ω&hl=ru&gl=RU&ceid=RU:ru"
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

try:
    response = requests.get(rss_url, headers=headers)
    feed = feedparser.parse(response.content)
    cutoff = datetime.now(timezone.utc) - timedelta(days=14)

    for entry in feed.entries:
        title = entry.title
        link = entry.link
        source = entry.get("source", {}).get("title", "Google News")
        pub_date = entry.get("published_parsed")

        if not pub_date:
            continue

        dt = datetime(*pub_date[:6])
        if dt < cutoff:
            continue

        formatted_date = dt.strftime("%d.%m.%Y %H:%M")
        google_news_articles.append([formatted_date, source, title, link])

    print(f"  ‚ûï –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(google_news_articles)} —Å—Ç–∞—Ç–µ–π –∏–∑ Google News RSS.")
except Exception as e:
    print("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ Google News:", e)

# --- –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å—ë –≤ CSV ---
all_articles = newsapi_articles + google_news_articles
csv_path = Path(__file__).parent / "data" / "megafon_news_combined.csv"
csv_path.parent.mkdir(parents=True, exist_ok=True)

with open(csv_path, mode="w", encoding="utf-8", newline="") as file:
    writer = csv.writer(file)
    writer.writerow(["date", "source", "title", "url"])
    writer.writerows(all_articles)

print(f"\n‚úÖ –í—Å–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(all_articles)} –Ω–æ–≤–æ—Å—Ç–µ–π –≤ —Ñ–∞–π–ª {csv_path}")
